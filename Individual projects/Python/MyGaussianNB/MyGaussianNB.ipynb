{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4dfa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe5b168",
   "metadata": {},
   "source": [
    "## Penguins_af.csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f0c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_af = pd.read_csv('penguins_af.csv', index_col = 0)\n",
    "penguins_af.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfe37a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_names = ['bill_length_mm', 'bill_depth_mm','flipper_length_mm', 'body_mass_g']\n",
    "penguins = penguins_af[f_names + ['species']]\n",
    "#two classes\n",
    "penguins2C = penguins.loc[penguins['species'].isin(['Adelie','Chinstrap'])]\n",
    "#three classes\n",
    "#penguins2C = penguins.loc[penguins['species'].isin(['Adelie','Chinstrap','Gentoo'])]\n",
    "#Changed the target values into numeric values.\n",
    "penguins2C['species'] = penguins2C['species'].astype('category')\n",
    "penguins2C['species']=penguins2C['species'].cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0273e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins2C['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e2636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y is the target value\n",
    "y = penguins2C.pop('species').values\n",
    "X_raw = penguins2C.values\n",
    "feature_names = penguins2C.columns\n",
    "X_tr_raw, X_ts_raw, y_train, y_test = train_test_split(X_raw, y, random_state=2, test_size=1/2)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_tr_raw)\n",
    "X_test = scaler.transform(X_ts_raw)\n",
    "max_k = X_train.shape[1]\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0df17",
   "metadata": {},
   "source": [
    "## My GaussianNB Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a177f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGaussianNB(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # set the number of samples and the number of features to the dataset shape\n",
    "        n_samples,n_features = X.shape\n",
    "        #get the number of classes the number of unique values of y\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        #initalise numpy arrays to hold the mean, variance and prior probability values. The shape is determined by \n",
    "        #the number of classes and the number of features.\n",
    "        self.mean = np.zeros((n_classes,n_features))\n",
    "        self.var = np.zeros((n_classes,n_features))\n",
    "        self.prior = np.zeros((n_classes))\n",
    "        \n",
    "        #for loop to get the correct values for the mean, variance and prior probability for each row the the dataset \n",
    "        #by the class they belong to.\n",
    "        for c in self.classes:\n",
    "            X_class= X[c==y]\n",
    "            #print(c,X_class)\n",
    "            self.mean[c,:] = X_class.mean(axis=0)\n",
    "            #print(\"mean\",self.mean)\n",
    "            self.var[c,:] = X_class.var(axis=0)\n",
    "            # prior probability calculated by dividing feature value by class by the total number of samples\n",
    "            #Calculating prior probability in this method as opposed to the predict method will mean the prior \n",
    "            #probability will be calculated on the training data.\n",
    "            self.prior[c] = X_class.shape[0]/n_samples\n",
    "            #print (self.prior)\n",
    "            \n",
    "\n",
    "    # calls the _predict function to make the prediction about what class the test dataset features belong to.\n",
    "    def predict(self,X):\n",
    "        y_pred=[self.posterior(x) for x in X]\n",
    "        return y_pred\n",
    "    \n",
    "    # A function to calculate the conditional probability and the posterior probability of each feature in the dataset\n",
    "    def posterior(self,X):\n",
    "        posterior_list=[]\n",
    "        for i, c in enumerate(self.classes):\n",
    "            prior = self.prior[i]\n",
    "            #calls the conditional probability function to calculate the conditional probability.\n",
    "            c_probability= self.conditional_probability(i,X)\n",
    "           #calculate the product of each conditional probability array of features\n",
    "            prod=np.prod(c_probability)\n",
    "            #add the product of conditional features to the prior probability to get the posterior probability.\n",
    "            posterior= prod + prior\n",
    "            posterior_list.append(posterior)\n",
    "        #return the maximum value from the posterior probability list and shows the class it has predicted the \n",
    "        #value belongs to.\n",
    "        return self.classes[np.argmax(posterior_list)]\n",
    "    \n",
    "    \n",
    "    #Function to calculate the conditional probability based on the given formula.        \n",
    "    def conditional_probability(self,class_i,x):\n",
    "        e= np.e\n",
    "        pi = np.pi\n",
    "        mean = self.mean[class_i]\n",
    "        var = self.var[class_i]\n",
    "        equation1= 1/(np.sqrt(2*pi*var))\n",
    "        numerator = (x-mean)**2\n",
    "        denom = 2*var\n",
    "        expo= np.exp(-(numerator/denom))\n",
    "        prob = equation1 * expo\n",
    "        return prob\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b13878",
   "metadata": {},
   "source": [
    "## Scikit-Learn GaussianNB Implementation on the penguins dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9213382",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X_train,y_train)\n",
    "y_pred=gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eb5068",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ca55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadc65cf",
   "metadata": {},
   "source": [
    "### MyGaussianNb implementation on penguins.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3621819",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgnb = MyGaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcbc34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573280ec",
   "metadata": {},
   "outputs": [],
   "source": [
    " y_pred=mgnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d2f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgnb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df12b2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45342c20",
   "metadata": {},
   "source": [
    "### Comparison of the two approaches\n",
    "Tested with a binary target Adelie or Chinstrap:<br>\n",
    "MyGaussianNB<br>\n",
    "Confusion matrix:<br>\n",
    "[[70  0]<br>\n",
    "[ 6 31]]<br>\n",
    " \n",
    "GaussianNB<br>\n",
    "Confusion matrix:<br>\n",
    "[[69  1]<br>\n",
    "[ 3 34]]<br>\n",
    "\n",
    "We can see from the ClassifierMixin.score which calculates the mean accuracy across the datset that GaussianNB performed better at predicting the appropriate class. The GaussianNB class had 1 flase positive and 3 false negatives. The MyGaussianNB class had no false positives making it better at predicting the correct class for this particular class, but it had 6 false negative which means it incorrectly labelled the type of penguin. \n",
    "\n",
    "Tested with all three class Adelie, Chinstrap or Gentoo:<br>\n",
    "Confusion matrix:<br>\n",
    "    a  c  g <br>\n",
    "a[[68  2  0]<br>\n",
    "c[ 4 30  0]<br>\n",
    "g[ 0  0 63]]<br>\n",
    " \n",
    "True positive Adelie is 68<br>\n",
    "False negative is 2<br>\n",
    "False positive is 4<br>\n",
    "True negative Adelie is 93<br>\n",
    "Chinstrap<br>\n",
    "True positive Chinstrap is 2<br>\n",
    "False negative is 4<br>\n",
    "False positive is 30<br>\n",
    "True negative Chinstrap is 131<br>\n",
    "Gentoo<br>\n",
    "True positive Gentoo is 0<br>\n",
    "False negative is 0<br>\n",
    "False positive is 63<br>\n",
    "True negative Gentoo is 104<br>\n",
    "\n",
    "\n",
    "So while the score looks to have improved slightly we can see from the confusion matrix that when we add in a third class Gentoo the predictive power gets worse and more false negatives and positives occur. This is probably due to limited Gentoo data and data that is more similar.\n",
    "\n",
    "When i changed the test parameter to 1/3 instead of 1/2 the confusion matrix inproved significantly. <br>\n",
    "Confusion matrix:<br>\n",
    "[[44  1  0]<br>\n",
    " [ 0 22  0]<br>\n",
    " [ 0  0 44]]<br>\n",
    " <br>\n",
    " \n",
    " and again when i made the test portion 1/4 of the dataset<br>\n",
    " Confusion matrix:<br>\n",
    "[[37  1  0]<br>\n",
    " [ 0 15  0]<br>\n",
    " [ 0  0 31]]<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1376f1",
   "metadata": {},
   "source": [
    "## Testing on further datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10469f5",
   "metadata": {},
   "source": [
    "### Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df469dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('diabetes.csv', index_col = False)\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b339e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_names = ['preg','plas', 'pres','skin', 'insu','mass','pedi','age']\n",
    "diabetes2C = diabetes[f_names + ['neg_pos']]\n",
    "diabetes2C['neg_pos'] = diabetes2C['neg_pos'].astype('category')\n",
    "diabetes2C['neg_pos']= diabetes2C['neg_pos'].cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63220231",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = diabetes2C.pop('neg_pos').values\n",
    "X_raw = diabetes2C.values\n",
    "feature_names = diabetes2C.columns\n",
    "X_tr_raw, X_ts_raw, y_train, y_test = train_test_split(X_raw, y, random_state=2, test_size=1/2)\n",
    "#Testing if training dataset was larger and testing portion was smaller do we get better outcomes?\n",
    "#X_tr_raw, X_ts_raw, y_train, y_test = train_test_split(X_raw, y, random_state=2, test_size=1/3)\n",
    "#X_tr_raw, X_ts_raw, y_train, y_test = train_test_split(X_raw, y, random_state=2, test_size=1/4)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_tr_raw)\n",
    "X_test = scaler.transform(X_ts_raw)\n",
    "max_k = X_train.shape[1]\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d6ccb5",
   "metadata": {},
   "source": [
    "### My Gaussian Naive Bayes prediction on the Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f380ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af455644",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=mgnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18c0e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgnb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f2276",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ea3ed",
   "metadata": {},
   "source": [
    "### Scikit learn Gaussian Naive Bayes example on the Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af41a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91604daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403bd028",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644114a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b40db84",
   "metadata": {},
   "source": [
    "This dataset does not perform as well as the penguins dataset with either the GaussianNb or the MyGaussianNB predictions mean accuracy falls from the mid 90's when predicting for the penguins dataset to the low 70's with this new data. We also get a lot more false positives and negatives in the confusion matrix.\n",
    "MyGaussianNB<br>\n",
    "Confusion matrix:<br>\n",
    "[[208  48]<br>\n",
    "[ 62  66]]<br>\n",
    "48 false positives and 62 false negatives this is much more serious that misclassifying penguins. As the people will eaither be recieving medication they don't need (the false positives) or not recieving the medication they do need false negatives.\n",
    "\n",
    "GaussianNb<br>\n",
    "Confusion matrix:<br>\n",
    "[[211  45]<br>\n",
    "[ 56  72]]<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74644d76",
   "metadata": {},
   "source": [
    "## GlassV2 example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "glass = pd.read_csv('glassV2.csv', index_col = False)\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f512a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_names = ['RI','Na', 'Mg','Al', 'Si','K','Ca','Ba','Fe']\n",
    "f_names = ['RI','Na', 'Mg','Al', 'Si','K','Ca']\n",
    "glass2C = glass[f_names + ['Type']]\n",
    "glass2C['Type'] = glass2C['Type'].astype('category')\n",
    "glass2C['Type']= glass2C['Type'].cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf0215",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = glass2C.pop('Type').values\n",
    "X_raw = glass2C.values\n",
    "feature_names = glass2C.columns\n",
    "X_tr_raw, X_ts_raw, y_train, y_test = train_test_split(X_raw, y, random_state=2, test_size=1/2)\n",
    "#Testing if training dataset was larger and testing portion was smaller do we get better outcomes?\n",
    "#X_tr_raw, X_ts_raw, y_train, y_test = train_test_split(X_raw, y, random_state=2, test_size=1/3)\n",
    "#X_tr_raw, X_ts_raw, y_train, y_test = train_test_split(X_raw, y, random_state=2, test_size=1/4)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_tr_raw)\n",
    "X_test = scaler.transform(X_ts_raw)\n",
    "max_k = X_train.shape[1]\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a9c14",
   "metadata": {},
   "source": [
    "### My Gaussian Naive Bayes prediction on the glassV2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d58382",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=mgnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d33fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgnb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3109ef95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58178d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ccafd4",
   "metadata": {},
   "source": [
    "### Scikit learn Gaussian Naive Bayes example on the gassV2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695e7ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb=GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f27cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8485fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b496e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056b259",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62076f19",
   "metadata": {},
   "source": [
    "I removed Fe and BA from the dataset for the purpose of testing the accuracy of MyGaussianNB as it did not handle divinging by zero gracefully and the predictive power was very low.<br>\n",
    "Confusion matrix:<br>\n",
    "[[ 0  0 35  0  0]<br>\n",
    " [ 0  0 34  0  0]<br>\n",
    " [ 0  0  7  0  0]<br>\n",
    " [ 0  0  9  0  0]<br>\n",
    " [ 0  0 18  0  0]]<br>\n",
    " \n",
    "With a mean accuracy of 0.068.\n",
    " \n",
    "The GaussianNB predictions seemed to handle this just fine:<br>\n",
    "Confusion matrix:<br>\n",
    "[[ 9  4 22  0  0]<br>\n",
    " [ 9 12 13  0  0]<br>\n",
    " [ 5  0  2  0  0]<br>\n",
    " [ 0  3  0  3  3]<br>\n",
    " [ 1  3  0  1 13]]<br>\n",
    " \n",
    " and a mean accuracy of 0.378.\n",
    " \n",
    "with the removal of the two fetures that consisted of mostly zero we see a marked improvement in the predictive power of MyGaussianNB with mean accuracy rising to 0.368 and the confusion matrix display more predictive capabilities.<br>\n",
    "Confusion matrix:<br>\n",
    "[[ 8  3 24  0  0]<br>\n",
    " [ 9 11 14  0  0]<br>\n",
    " [ 5  0  2  0  0]<br>\n",
    " [ 0  5  0  4  0]<br>\n",
    " [ 1  4  0  0 13]]<br>\n",
    " \n",
    "The GaussianNB implementation remains relatively unchanged leading me to believe that it handles data that results in division by zero, so that it does not affect the prediction. \n",
    "\n",
    "Testing with different train/test data splits:<br>\n",
    "When we give a larger training set this dataset is different to the two previous ones in that the predictive power goes down and the predictions get worse with lower True positives and True negatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e480c",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The GaussianNB class has out performed the MyGaussianNB implementation But only slightly across the various datasets. What was more evident was that the GaussianNB implementation accounted for things like values being divided by zero while mine did not. Trying differnt feature combinations did not yield much success except in the case of removing the two mostly zero value columns in the Diabetes data. While sometimes initially you noticed a better mean average score when you looked at the confusion matrix sometimes things deteriorate with more false positives and negatives. So it's important to look at a few evaluation methods and not rely on just the mean average score but it was helpful in developing the MyGaussianNB as it was a quick way to check you were on the right track.<br>\n",
    "The worst performing of the three datasets was GlassV2 and we can see a trend between the three dataset as we process them. The independance of the features seems to decrease from penguins to glassV2 and this can cause Naive Gaussian Bayes to perform poorly. The backbone of Naive Bayes is that features are independent of one another but as we've seen thats not the case with all datasets. So given the three datasets in the future I would look at other prediction models for both the Diabetes and Glass V2 datasets to see if improvement could be found with another model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
